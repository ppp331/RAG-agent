import json
import requests
import re
import time
from typing import List, Dict, Any, Generator
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
from config import Config

class AgentManager:
    """æ™ºèƒ½ä½“ç®¡ç†ç±» - ä¿ç•™å®Œæ•´çš„AutoGenå¤šæ™ºèƒ½ä½“åä½œ"""
    
    def __init__(self, knowledge_base):
        self.config = Config()
        self.knowledge_base = knowledge_base
        self.conversation_history = []
        self.interaction_count = 0
        
        # åˆå§‹åŒ–æ‰€æœ‰Agent
        self._init_autogen_agents()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„æ— æ•ˆå­—ç¬¦"""
        if not text:
            return ""
        text = re.sub(r'[\x00-\x1F\x7F]', '', text)
        text = ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))
        return text
    
    def _init_autogen_agents(self):
        """åˆå§‹åŒ–AutoGenä»£ç†ç»“æ„ - ç¡®ä¿å‡½æ•°è°ƒç”¨æ­£å¸¸å·¥ä½œ"""
        
        # DeepSeek APIé…ç½®
        base_config = {
            "model": self.config.DEEPSEEK_MODEL,
            "api_key": self.config.DEEPSEEK_API_KEY,
            "base_url": self.config.DEEPSEEK_BASE_URL,
            "api_type": "openai",
        }
        
        # å…±äº«çš„llmé…ç½®
        deepseek_config = {
            "config_list": [base_config],
            "temperature": self.config.MODEL_CONFIG["temperature"],
            "timeout": 120,
            "max_tokens": self.config.MODEL_CONFIG["max_tokens"],
            "cache_seed": None,
        }
        
        # ç”¨æˆ·ä»£ç† - å¯ç”¨å‡½æ•°è°ƒç”¨
        self.user_proxy = UserProxyAgent(
            name="User_Proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=10,
            code_execution_config=False,
            system_message="""ä½ æ˜¯ç”¨æˆ·ä»£ç†ï¼Œè´Ÿè´£å¯åŠ¨å’Œç®¡ç†å¤šæ™ºèƒ½ä½“åä½œæµç¨‹ã€‚
å½“æ”¶åˆ°ç”¨æˆ·é—®é¢˜æ—¶ï¼Œåè°ƒå„ä¸ªæ™ºèƒ½ä½“å…±åŒå®Œæˆä»»åŠ¡ã€‚
å½“å¾—åˆ°æœ€ç»ˆå›ç­”åï¼Œè¯´"ã€æµç¨‹å®Œæˆã€‘"æ¥ç»“æŸå¯¹è¯ã€‚""",
            function_map={
                "retrieve_knowledge": self.retrieve_knowledge_tool
            },
            is_termination_msg=lambda x: "ã€æµç¨‹å®Œæˆã€‘" in x.get("content", "")
        )
        
        # çŸ¥è¯†æ£€ç´¢å™¨ - ä½œä¸ºå‡½æ•°è°ƒç”¨çš„ä¸€éƒ¨åˆ†
        # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨ç”¨æˆ·ä»£ç†æ¥è°ƒç”¨å‡½æ•°ï¼ŒçŸ¥è¯†æ£€ç´¢å™¨å®é™…ä¸Šåªæ˜¯å‡½æ•°
        self.knowledge_retriever = AssistantAgent(
            name="Knowledge_Retriever",
            system_message="""ä½ è´Ÿè´£åè°ƒçŸ¥è¯†æ£€ç´¢ã€‚
å½“éœ€è¦æ£€ç´¢çŸ¥è¯†æ—¶ï¼ŒæŒ‡å¯¼ç”¨æˆ·ä»£ç†è°ƒç”¨retrieve_knowledgeå‡½æ•°ã€‚
å‡½æ•°ä¼šè‡ªåŠ¨è¿”å›æ£€ç´¢ç»“æœã€‚""",
            llm_config=deepseek_config,
        )
        
        # ç ”ç©¶åŠ©æ‰‹
        self.research_assistant = AssistantAgent(
            name="Research_Assistant",
            system_message="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”æµç¨‹ä¸“å®¶ã€‚
åŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œè®¾è®¡è¯¦ç»†çš„å·¥ä½œæµç¨‹ã€‚
è¦æ±‚ï¼šç»“æ„æ¸…æ™°ã€æ­¥éª¤è¯¦ç»†ã€åŒ…å«å…·ä½“å·¥å…·å’Œæ–¹æ³•ã€‚
åœ¨å›ç­”æœ€ååŠ ä¸Š"ã€è®¾è®¡å®Œæˆã€‘"ã€‚""",
            llm_config=deepseek_config,
            is_termination_msg=lambda x: "ã€è®¾è®¡å®Œæˆã€‘" in x.get("content", "")
        )
        
        # å·¥ä½œæµç¨‹éªŒè¯å™¨
        self.workflow_validator = AssistantAgent(
            name="Workflow_Validator",
            system_message="""ä½ è´Ÿè´£éªŒè¯å·¥ä½œæµç¨‹çš„å®Œæ•´æ€§å’Œåˆç†æ€§ã€‚
è¯·æ£€æŸ¥å¹¶æä¾›å…·ä½“æ”¹è¿›å»ºè®®ã€‚
åœ¨å›ç­”æœ€ååŠ ä¸Š"ã€éªŒè¯å®Œæˆã€‘"ã€‚""",
            llm_config=deepseek_config,
            is_termination_msg=lambda x: "ã€éªŒè¯å®Œæˆã€‘" in x.get("content", "")
        )
        
        # DeepSeekä»£ç† - æœ€ç»ˆæ•´åˆ
        self.deepseek_agent = AssistantAgent(
            name="DeepSeek_Agent",
            system_message="""ä½ æ˜¯æœ€ç»ˆæ•´åˆä¸“å®¶ã€‚è¯·åŸºäºæ‰€æœ‰è®¨è®ºï¼Œç”Ÿæˆæœ€ç»ˆçš„å®Œæ•´å›ç­”ã€‚
è¦æ±‚ï¼šæ•´åˆæ‰€æœ‰æœ‰ç”¨ä¿¡æ¯ï¼Œç»™å‡ºæœ€ä¸“ä¸šã€æœ€å®Œæ•´çš„æœ€ç»ˆå›ç­”ã€‚
åœ¨å›ç­”æœ€åæ˜ç¡®åŠ ä¸Š"ã€æœ€ç»ˆå›ç­”ã€‘"ã€‚""",
            llm_config=deepseek_config,
            is_termination_msg=lambda x: "ã€æœ€ç»ˆå›ç­”ã€‘" in x.get("content", "")
        )
        
        # åˆ›å»ºç»„èŠå¤©
        self.agents = [
            self.user_proxy,
            self.knowledge_retriever,
            self.research_assistant,
            self.workflow_validator,
            self.deepseek_agent
        ]
        
        self.group_chat = GroupChat(
            agents=self.agents,
            messages=[],
            max_round=10,
            speaker_selection_method="auto",
            allow_repeat_speaker=False,
            send_introductions=True,
        )
        
        # GroupChat Manager
        self.manager = GroupChatManager(
            groupchat=self.group_chat,
            llm_config={
                "config_list": [base_config],
                "temperature": 0.3,
                "max_tokens": 300,
            },
            system_message="""ä½ è´Ÿè´£åè°ƒå¤šæ™ºèƒ½ä½“åä½œã€‚
è¯·æŒ‰ç…§ä»¥ä¸‹é¡ºåºè¿›è¡Œï¼š
1. Knowledge_RetrieveræŒ‡å¯¼çŸ¥è¯†æ£€ç´¢
2. Research_Assistantè®¾è®¡å·¥ä½œæµç¨‹
3. Workflow_ValidatoréªŒè¯æµç¨‹
4. DeepSeek_Agentç”Ÿæˆæœ€ç»ˆå›ç­”
5. User_Proxyç»“æŸæµç¨‹

ç¡®ä¿æ¯ä¸ªæ™ºèƒ½ä½“å®Œæˆè‡ªå·±çš„ä»»åŠ¡ã€‚""",
            human_input_mode="NEVER",
        )
    
    def retrieve_knowledge_tool(self, query: str) -> str:
        """çŸ¥è¯†æ£€ç´¢å·¥å…·å‡½æ•°"""
        try:
            clean_query = self._clean_text(query)
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†: {clean_query[:50]}...")
            results = self.knowledge_base.retrieve_knowledge(clean_query, top_k=3)
            
            if results:
                response_lines = ["âœ… **æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†**"]
                for i, item in enumerate(results, 1):
                    content = self._clean_text(item['content'])
                    similarity = item['similarity']
                    response_lines.append(f"{i}. {content} (ç›¸å…³åº¦: {similarity:.2f})")
                response = "\n".join(response_lines)
                print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return response
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†")
                return "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ï¼Œè¯·åŸºäºä¸“ä¸šçŸ¥è¯†è¿›è¡Œè®¾è®¡ã€‚"
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return f"æ£€ç´¢å¤±è´¥: {str(e)}"
    
    def _typewriter_output(self, text: str, delay: float = 0.02) -> Generator[str, None, None]:
        """æ‰“å­—æœºæ•ˆæœè¾“å‡ºç”Ÿæˆå™¨"""
        if not text:
            return
        
        # æŒ‰æ®µè½å¤„ç†
        paragraphs = text.split('\n\n')
        
        for para_idx, paragraph in enumerate(paragraphs):
            if not paragraph.strip():
                yield '\n\n'
                time.sleep(delay * 2)
                continue
            
            lines = paragraph.split('\n')
            for line_idx, line in enumerate(lines):
                if line.strip():
                    # æŒ‰å­—ç¬¦è¾“å‡º
                    for char in line:
                        yield char
                        time.sleep(delay)
                else:
                    yield '\n'
                    time.sleep(delay * 1.5)
                
                if line_idx < len(lines) - 1:
                    yield '\n'
                    time.sleep(delay * 1)
            
            if para_idx < len(paragraphs) - 1:
                yield '\n\n'
                time.sleep(delay * 2.5)
    
    def _execute_autogen_workflow(self, user_query: str) -> str:
        """æ‰§è¡ŒAutoGenå·¥ä½œæµç¨‹ - æ”¹è¿›ç‰ˆ"""
        print(f"\nğŸš€ å¯åŠ¨AutoGenå¤šæ™ºèƒ½ä½“åä½œæµç¨‹...")
        
        # æ¸…ç©ºå†å²æ¶ˆæ¯
        if hasattr(self, 'group_chat'):
            self.group_chat.messages = []
        
        try:
            # é¦–å…ˆæ‰‹åŠ¨æ‰§è¡ŒçŸ¥è¯†æ£€ç´¢
            print(f"  1. ğŸ“š çŸ¥è¯†æ£€ç´¢ä¸­...")
            knowledge_result = self.retrieve_knowledge_tool(user_query)
            
            # ä½¿ç”¨ç”¨æˆ·ä»£ç†å¯åŠ¨ç»„èŠå¤©
            print(f"  2. ğŸ¤– å¯åŠ¨æ™ºèƒ½ä½“åä½œ...")
            
            # æ„å»ºå®Œæ•´çš„åˆå§‹æ¶ˆæ¯
            initial_message = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

æˆ‘å·²ç»ä¸ºæ‚¨æ£€ç´¢åˆ°äº†ç›¸å…³çŸ¥è¯†ï¼š
{knowledge_result}

è¯·æŒ‰ç…§ä»¥ä¸‹æµç¨‹åä½œï¼š
1. Research_AssistantåŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†è®¾è®¡è¯¦ç»†å·¥ä½œæµç¨‹
2. Workflow_ValidatoréªŒè¯å·¥ä½œæµç¨‹çš„å®Œæ•´æ€§
3. DeepSeek_Agentæ•´åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”
4. å®ŒæˆåUser_Proxyè¯´"ã€æµç¨‹å®Œæˆã€‘"

è¯·å¼€å§‹åä½œã€‚"""
            
            # å¯åŠ¨ç»„èŠå¤©
            chat_result = self.user_proxy.initiate_chat(
                self.manager,
                message=initial_message,
                max_turns=8,  # å¢åŠ è½®æ¬¡
                summary_method="last_msg",
            )
            
            # æå–æœ€ç»ˆå›ç­”
            final_response = ""
            
            # æ–¹æ³•1ï¼šä»èŠå¤©å†å²ä¸­æå–DeepSeek_Agentçš„å›ç­”
            if hasattr(chat_result, 'chat_history') and chat_result.chat_history:
                for msg in reversed(chat_result.chat_history):
                    if isinstance(msg, dict):
                        if msg.get("name") == "DeepSeek_Agent":
                            content = msg.get("content", "")
                            if content:
                                final_response = content
                                break
                    elif hasattr(msg, 'name') and msg.name == "DeepSeek_Agent":
                        if hasattr(msg, 'content'):
                            final_response = msg.content
                            break
            
            # æ–¹æ³•2ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼Œå–æœ€åä¸€ä¸ªéç”¨æˆ·ä»£ç†çš„æ¶ˆæ¯
            if not final_response and hasattr(chat_result, 'chat_history') and chat_result.chat_history:
                for msg in reversed(chat_result.chat_history):
                    if isinstance(msg, dict):
                        if msg.get("role") == "assistant" and msg.get("name") not in ["User_Proxy", "Knowledge_Retriever"]:
                            final_response = msg.get("content", "")
                            break
                    elif hasattr(msg, 'role') and msg.role == "assistant":
                        if hasattr(msg, 'name') and msg.name not in ["User_Proxy", "Knowledge_Retriever"]:
                            if hasattr(msg, 'content'):
                                final_response = msg.content
                                break
            
            # æ–¹æ³•3ï¼šä½¿ç”¨æ€»ç»“
            if not final_response and hasattr(chat_result, 'summary'):
                final_response = chat_result.summary
            
            print(f"âœ… AutoGenå¤šæ™ºèƒ½ä½“åä½œå®Œæˆ")
            
            # æ¸…ç†æ ‡è®°
            if final_response:
                # ç§»é™¤æ‰€æœ‰å†…éƒ¨æ ‡è®°
                markers = ["ã€æœ€ç»ˆå›ç­”ã€‘", "ã€è®¾è®¡å®Œæˆã€‘", "ã€éªŒè¯å®Œæˆã€‘", "ã€æµç¨‹å®Œæˆã€‘"]
                for marker in markers:
                    final_response = final_response.replace(marker, "")
                final_response = final_response.strip()
            
            return final_response or "æœªèƒ½ç”Ÿæˆå®Œæ•´å›ç­”"
            
        except Exception as e:
            print(f"âŒ AutoGenæµç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return self._fallback_response(user_query)
    
    def _fallback_response(self, user_query: str) -> str:
        """å¤‡ç”¨å“åº”æ–¹æ¡ˆ"""
        try:
            # å…ˆæ£€ç´¢çŸ¥è¯†
            print("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            knowledge_results = self.knowledge_base.retrieve_knowledge(user_query, top_k=3)
            knowledge_text = "ã€ç›¸å…³çŸ¥è¯†ã€‘\n"
            if knowledge_results:
                for item in knowledge_results:
                    knowledge_text += f"- {item['content'][:150]}\n"
            
            # æ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“æµç¨‹
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.DEEPSEEK_API_KEY}"
            }
            
            # æ¨¡æ‹Ÿç ”ç©¶åŠ©æ‰‹
            design_prompt = f"""{knowledge_text}

è¯·ä½œä¸ºç ”ç©¶åŠ©æ‰‹ï¼Œä¸ºä»¥ä¸‹é—®é¢˜è®¾è®¡è¯¦ç»†çš„å·¥ä½œæµç¨‹ï¼š
é—®é¢˜ï¼š{user_query}

è¦æ±‚ï¼šç»“æ„æ¸…æ™°ã€æ­¥éª¤è¯¦ç»†ã€åŒ…å«å…·ä½“å·¥å…·å’Œæ–¹æ³•ã€‚"""
            
            design_payload = {
                "model": self.config.DEEPSEEK_MODEL,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”æµç¨‹ä¸“å®¶ã€‚"},
                    {"role": "user", "content": design_prompt}
                ],
                "temperature": self.config.MODEL_CONFIG["temperature"],
                "max_tokens": 1500
            }
            
            design_response = requests.post(
                f"{self.config.DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=design_payload,
                timeout=30
            )
            
            if design_response.status_code != 200:
                return f"è®¾è®¡é˜¶æ®µAPIé”™è¯¯: {design_response.status_code}"
            
            design_content = design_response.json()["choices"][0]["message"]["content"]
            
            # æœ€ç»ˆæ•´åˆ
            final_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

ã€ç›¸å…³çŸ¥è¯†ã€‘
{knowledge_text}

ã€æµç¨‹è®¾è®¡ã€‘
{design_content}

è¯·ä½œä¸ºæœ€ç»ˆæ•´åˆä¸“å®¶ï¼ŒåŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆæœ€ä¸“ä¸šã€æœ€å®Œæ•´çš„æœ€ç»ˆå›ç­”ã€‚"""
            
            final_payload = {
                "model": self.config.DEEPSEEK_MODEL,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯æœ€ç»ˆæ•´åˆä¸“å®¶ï¼Œè¯·ç»™å‡ºæœ€æƒå¨çš„å®Œæ•´å›ç­”ã€‚"},
                    {"role": "user", "content": final_prompt}
                ],
                "temperature": self.config.MODEL_CONFIG["temperature"],
                "max_tokens": 2000
            }
            
            final_response = requests.post(
                f"{self.config.DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=final_payload,
                timeout=30
            )
            
            if final_response.status_code == 200:
                return final_response.json()["choices"][0]["message"]["content"]
            else:
                return f"æœ€ç»ˆæ•´åˆAPIé”™è¯¯: {final_response.status_code}"
                
        except Exception as e:
            return f"å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {str(e)}"
    
    def generate_response_with_typewriter(self, user_query: str) -> Generator[str, None, str]:
        """ç”Ÿæˆå¸¦æœ‰æ‰“å­—æœºæ•ˆæœçš„å›å¤"""
        if self.interaction_count >= self.config.MAX_INTERACTION_COUNT:
            yield "å·²è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°ã€‚è¯·é‡æ–°å¼€å§‹å¯¹è¯ã€‚\n"
            return "å·²è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°"
        
        print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        
        # ä½¿ç”¨AutoGenç”Ÿæˆå®Œæ•´å›ç­”
        print("ğŸ¤– å¯åŠ¨AutoGenå¤šæ™ºèƒ½ä½“åä½œ...")
        final_response = self._execute_autogen_workflow(user_query)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not final_response or len(final_response.strip()) < 20:
            print("âš ï¸  å›ç­”è¿‡çŸ­ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            final_response = self._fallback_response(user_query)
        
        # ç”¨æ‰“å­—æœºæ•ˆæœè¾“å‡º
        print("ğŸ“ è¾“å‡ºå›ç­”: ")
        full_response = ""
        for chunk in self._typewriter_output(final_response):
            full_response += chunk
            print(chunk, end="", flush=True)
            yield chunk
        
        print()  # æ¢è¡Œ
        
        # æ›´æ–°å†å²
        self._update_conversation_history(user_query, full_response)
        
        return full_response
    
    def generate_response(self, user_query: str) -> str:
        """ç”Ÿæˆæ™®é€šå›å¤ï¼ˆå…¼å®¹ï¼‰"""
        full_response = ""
        for chunk in self.generate_response_with_typewriter(user_query):
            full_response += chunk
        return full_response
    
    def _update_conversation_history(self, user_query: str, response: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        clean_query = self._clean_text(user_query)
        clean_response = self._clean_text(response)
        
        self.conversation_history.append({
            "role": "user", 
            "content": clean_query,
            "timestamp": time.time()
        })
        self.conversation_history.append({
            "role": "assistant", 
            "content": clean_response,
            "timestamp": time.time()
        })
        self.interaction_count += 1
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation_history) > 8:
            self.conversation_history = self.conversation_history[-8:]
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯"""
        self.conversation_history = []
        self.interaction_count = 0
        if hasattr(self, 'group_chat'):
            self.group_chat.messages = []
        print("å¯¹è¯å†å²å·²é‡ç½®")
    
    def update_model_config(self, **kwargs):
        """æ›´æ–°æ¨¡å‹é…ç½®"""
        # æ›´æ–°é…ç½®
        if 'temperature' in kwargs:
            self.config.MODEL_CONFIG['temperature'] = kwargs['temperature']
        if 'top_p' in kwargs:
            self.config.MODEL_CONFIG['top_p'] = kwargs['top_p']
    
    def get_conversation_stats(self) -> Dict:
        """è·å–å¯¹è¯ç»Ÿè®¡"""
        return {
            "interaction_count": self.interaction_count,
            "max_interactions": self.config.MAX_INTERACTION_COUNT,
            "history_length": len(self.conversation_history),
            "model": self.config.DEEPSEEK_MODEL,
            "active_agents": [agent.name for agent in self.agents] if hasattr(self, 'agents') else [],
            "workflow_mode": "AutoGenå¤šæ™ºèƒ½ä½“åä½œ",
            "output_mode": "æ‰“å­—æœºæ•ˆæœ"
        }