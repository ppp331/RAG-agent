import json
import requests
import re
import time
from typing import List, Dict, Any, Generator
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
from config import Config

class AgentManager:
    """æ™ºèƒ½ä½“ç®¡ç†ç±» - æœ€å°åŒ–ä¿®å¤ï¼Œä¿ç•™æ‰€æœ‰åŸæœ‰åŠŸèƒ½"""
    
    def __init__(self, knowledge_base):
        self.config = Config()
        self.knowledge_base = knowledge_base
        self.conversation_history = []
        self.interaction_count = 0
        
        # åˆå§‹åŒ–æ‰€æœ‰Agent
        self._init_autogen_agents()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„æ— æ•ˆå­—ç¬¦"""
        if not text:
            return ""
        text = re.sub(r'[\x00-\x1F\x7F]', '', text)
        text = ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))
        return text
    
    def _init_autogen_agents(self):
        """åˆå§‹åŒ–AutoGenä»£ç†ç»“æ„ - åªä¿®å¤é…ç½®æ ¼å¼ï¼Œä¸æ”¹å˜æ¶æ„"""
        
        # DeepSeek APIé…ç½®
        base_config = {
            "model": self.config.DEEPSEEK_MODEL,
            "api_key": self.config.DEEPSEEK_API_KEY,
            "base_url": self.config.DEEPSEEK_BASE_URL,
            "api_type": "openai",
        }
        
        # å…±äº«çš„llmé…ç½®
        deepseek_config = {
            "config_list": [base_config],
            "temperature": self.config.MODEL_CONFIG["temperature"],
            "timeout": 120,
            "max_tokens": self.config.MODEL_CONFIG["max_tokens"],
        }
        
        # ç”¨æˆ·ä»£ç† - ä¿æŒåŸæœ‰é€»è¾‘
        self.user_proxy = UserProxyAgent(
            name="User_Proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=0,
            code_execution_config=False,
            system_message="ä½ æ˜¯ç”¨æˆ·çš„ä»£ç†ï¼Œåªè´Ÿè´£æå‡ºç”¨æˆ·çš„é—®é¢˜ã€‚ä¸è¦è‡ªå·±å›ç­”é—®é¢˜ã€‚",
        )
        
        # æµç¨‹åè°ƒå™¨ - ä¿æŒåŸæœ‰é€»è¾‘
        self.workflow_coordinator = AssistantAgent(
            name="Workflow_Coordinator",
            system_message="""ä½ æ˜¯æµç¨‹åè°ƒå™¨ï¼Œè´Ÿè´£ç®¡ç†å¤šæ™ºèƒ½ä½“åä½œæµç¨‹ã€‚
ä½ çš„èŒè´£ï¼š
1. æ¥æ”¶ç”¨æˆ·é—®é¢˜
2. åè°ƒçŸ¥è¯†æ£€ç´¢ã€æµç¨‹è®¾è®¡ã€éªŒè¯å’Œæœ€ç»ˆæ•´åˆ
3. ç¡®ä¿æµç¨‹æŒ‰é¡ºåºè¿›è¡Œ
4. åœ¨æµç¨‹ç»“æŸæ—¶è¯´"ã€æµç¨‹å®Œæˆã€‘"

å…·ä½“æ­¥éª¤ï¼š
1. é¦–å…ˆæŒ‡å¯¼çŸ¥è¯†æ£€ç´¢
2. ç„¶åæŒ‡å¯¼æµç¨‹è®¾è®¡
3. æ¥ç€æŒ‡å¯¼æµç¨‹éªŒè¯
4. æœ€åæŒ‡å¯¼æœ€ç»ˆæ•´åˆ
5. æµç¨‹å®Œæˆåè¯´"ã€æµç¨‹å®Œæˆã€‘"å¹¶ç»“æŸ""",
            llm_config=deepseek_config,
        )
        
        # çŸ¥è¯†æ£€ç´¢å™¨ - å…³é”®ä¿®å¤ï¼šä¿®æ­£å‡½æ•°è°ƒç”¨é…ç½®æ ¼å¼
        self.knowledge_retriever = AssistantAgent(
            name="Knowledge_Retriever",
            system_message="""ä½ æ˜¯çŸ¥è¯†æ£€ç´¢ä¸“å®¶ã€‚å½“æµç¨‹åè°ƒå™¨è¦æ±‚æ£€ç´¢çŸ¥è¯†æ—¶ï¼š
1. è°ƒç”¨retrieve_knowledgeå·¥å…·å‡½æ•°è·å–ç›¸å…³çŸ¥è¯†
2. åˆ†ææ£€ç´¢ç»“æœ
3. æä¾›çŸ¥è¯†æ€»ç»“
4. å®Œæˆåè¯´"ã€çŸ¥è¯†æ£€ç´¢å®Œæˆã€‘"ä»¥ä¾¿æµç¨‹ç»§ç»­""",
            llm_config={
                "config_list": [base_config],  # åªåŒ…å«base_config
                "functions": [
                    {
                        "name": "retrieve_knowledge",
                        "description": "ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³çŸ¥è¯†",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {
                                    "type": "string",
                                    "description": "æ£€ç´¢æŸ¥è¯¢"
                                }
                            },
                            "required": ["query"]
                        }
                    }
                ],
                # ä¿®å¤ï¼šç§»é™¤å¯èƒ½å¯¼è‡´éªŒè¯é”™è¯¯çš„å‚æ•°
                "temperature": 0.1,
                "max_tokens": 500,
            },
            function_map={
                "retrieve_knowledge": self.retrieve_knowledge_tool
            },
        )
        
        # ç ”ç©¶åŠ©æ‰‹ - ä¿æŒåŸæœ‰é€»è¾‘
        self.research_assistant = AssistantAgent(
            name="Research_Assistant",
            system_message="""ä½ æ˜¯ç§‘ç ”æµç¨‹è®¾è®¡ä¸“å®¶ã€‚åŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œè®¾è®¡è¯¦ç»†çš„å·¥ä½œæµç¨‹ã€‚
è¦æ±‚ï¼š
1. ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨Markdownæ ¼å¼
2. æ­¥éª¤è¯¦ç»†å…·ä½“
3. åŒ…å«å¿…è¦çš„å·¥å…·å’Œæ–¹æ³•æ¨è
4. å®Œæˆåè¯´"ã€è®¾è®¡å®Œæˆã€‘"ä»¥ä¾¿æµç¨‹ç»§ç»­""",
            llm_config=deepseek_config,
        )
        
        # å·¥ä½œæµç¨‹éªŒè¯å™¨ - ä¿æŒåŸæœ‰é€»è¾‘
        self.workflow_validator = AssistantAgent(
            name="Workflow_Validator",
            system_message="""ä½ æ˜¯æµç¨‹éªŒè¯ä¸“å®¶ã€‚è¯·æ£€æŸ¥å·¥ä½œæµç¨‹çš„å®Œæ•´æ€§å’Œåˆç†æ€§ã€‚
æ£€æŸ¥è¦ç‚¹ï¼š
1. æ˜¯å¦æœ‰é—æ¼çš„å…³é”®æ­¥éª¤
2. é€»è¾‘é¡ºåºæ˜¯å¦åˆç†
3. æ¨èå·¥å…·æ˜¯å¦åˆé€‚
4. æŠ€æœ¯ç»†èŠ‚æ˜¯å¦æ­£ç¡®
è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
å®Œæˆåè¯´"ã€éªŒè¯å®Œæˆã€‘"ä»¥ä¾¿æµç¨‹ç»§ç»­""",
            llm_config=deepseek_config,
        )
        
        # DeepSeekä»£ç† - ä¿æŒåŸæœ‰é€»è¾‘
        self.deepseek_agent = AssistantAgent(
            name="DeepSeek_Agent",
            system_message="""ä½ æ˜¯æœ€ç»ˆæ•´åˆä¸“å®¶ã€‚è¯·åŸºäºæ‰€æœ‰è®¨è®ºï¼Œç”Ÿæˆæœ€ç»ˆçš„å®Œæ•´å›ç­”ã€‚
è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰æœ‰ç”¨ä¿¡æ¯
2. ç»™å‡ºæœ€ä¸“ä¸šã€æœ€å®Œæ•´çš„æœ€ç»ˆå›ç­”
3. ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨Markdownæ ¼å¼
4. åŒ…å«å…·ä½“æ­¥éª¤ã€å·¥å…·ã€æ³¨æ„äº‹é¡¹
5. ç›´æ¥é¢å‘ç”¨æˆ·ï¼Œä¸è¦æåŠå†…éƒ¨è®¨è®ºè¿‡ç¨‹
6. å®Œæˆåè¯´"ã€æœ€ç»ˆå›ç­”ã€‘"ä»¥ä¾¿æµç¨‹ç»“æŸ""",
            llm_config=deepseek_config,
        )
        
        # åˆ›å»ºç»„èŠå¤© - ä¿æŒåŸæœ‰é€»è¾‘
        self.agents = [
            self.user_proxy,           # ä»£è¡¨ç”¨æˆ·æå‡ºé—®é¢˜
            self.workflow_coordinator, # åè°ƒæ•´ä¸ªæµç¨‹
            self.knowledge_retriever,  # æ£€ç´¢çŸ¥è¯†
            self.research_assistant,   # è®¾è®¡æµç¨‹
            self.workflow_validator,   # éªŒè¯æµç¨‹
            self.deepseek_agent,       # æœ€ç»ˆå›ç­”
        ]
        
        self.group_chat = GroupChat(
            agents=self.agents,
            messages=[],
            max_round=12,
            speaker_selection_method="auto",
            allow_repeat_speaker=False,
            send_introductions=False,
        )
        
        # GroupChat Manager - ä¿æŒåŸæœ‰é€»è¾‘
        self.manager = GroupChatManager(
            groupchat=self.group_chat,
            llm_config={
                "config_list": [base_config],
                "temperature": 0.2,
                "max_tokens": 300,
            },
            system_message="""ä½ æ˜¯å¤šæ™ºèƒ½ä½“åä½œç®¡ç†å™¨ã€‚è¯·åè°ƒæ™ºèƒ½ä½“åä½œã€‚

æ™ºèƒ½ä½“åŠå…¶èŒè´£ï¼š
1. User_Proxyï¼šä»£è¡¨ç”¨æˆ·æå‡ºåˆå§‹é—®é¢˜
2. Workflow_Coordinatorï¼šåè°ƒæ•´ä¸ªå·¥ä½œæµç¨‹
3. Knowledge_Retrieverï¼šæ£€ç´¢ç›¸å…³çŸ¥è¯†
4. Research_Assistantï¼šè®¾è®¡å·¥ä½œæµç¨‹
5. Workflow_Validatorï¼šéªŒè¯æµç¨‹å®Œæ•´æ€§
6. DeepSeek_Agentï¼šç”Ÿæˆæœ€ç»ˆå›ç­”

è¯·ç¡®ä¿å¯¹è¯æœ‰åºè¿›è¡Œã€‚""",
            human_input_mode="NEVER",
        )
    
    def retrieve_knowledge_tool(self, query: str) -> str:
        """çŸ¥è¯†æ£€ç´¢å·¥å…·å‡½æ•° - ä¿æŒåŸæœ‰é€»è¾‘"""
        try:
            clean_query = self._clean_text(query)
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†: {clean_query[:50]}...")
            results = self.knowledge_base.retrieve_knowledge(clean_query, top_k=3)
            
            if results:
                response_lines = ["âœ… **æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†**"]
                for i, item in enumerate(results, 1):
                    content = self._clean_text(item['content'])
                    similarity = item['similarity']
                    response_lines.append(f"{i}. {content} (ç›¸å…³åº¦: {similarity:.2f})")
                response_lines.append("\nã€çŸ¥è¯†æ£€ç´¢å®Œæˆã€‘")
                response = "\n".join(response_lines)
                print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return response
            else:
                response = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ï¼Œè¯·åŸºäºä¸“ä¸šçŸ¥è¯†è¿›è¡Œè®¾è®¡ã€‚\nã€çŸ¥è¯†æ£€ç´¢å®Œæˆã€‘"
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†")
                return response
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return f"æ£€ç´¢å¤±è´¥: {str(e)}\nã€çŸ¥è¯†æ£€ç´¢å®Œæˆã€‘"
    
    def _typewriter_output(self, text: str, delay: float = 0.02) -> Generator[str, None, None]:
        """æ‰“å­—æœºæ•ˆæœè¾“å‡ºç”Ÿæˆå™¨ - ä¿æŒåŸæœ‰é€»è¾‘"""
        if not text:
            return
        
        # æŒ‰æ®µè½å¤„ç†
        paragraphs = text.split('\n\n')
        
        for para_idx, paragraph in enumerate(paragraphs):
            if not paragraph.strip():
                yield '\n\n'
                time.sleep(delay * 2)
                continue
            
            lines = paragraph.split('\n')
            for line_idx, line in enumerate(lines):
                if line.strip():
                    # æŒ‰å­—ç¬¦è¾“å‡º
                    for char in line:
                        yield char
                        time.sleep(delay)
                else:
                    yield '\n'
                    time.sleep(delay * 1.5)
                
                if line_idx < len(lines) - 1:
                    yield '\n'
                    time.sleep(delay * 1)
            
            if para_idx < len(paragraphs) - 1:
                yield '\n\n'
                time.sleep(delay * 2.5)
    
    def _execute_autogen_workflow(self, user_query: str) -> str:
        """æ‰§è¡ŒAutoGenå·¥ä½œæµç¨‹ - ä¿æŒåŸæœ‰é€»è¾‘"""
        print(f"\nğŸš€ å¯åŠ¨AutoGenå¤šæ™ºèƒ½ä½“åä½œæµç¨‹...")
        
        # æ¸…ç©ºå†å²æ¶ˆæ¯
        if hasattr(self, 'group_chat'):
            self.group_chat.messages = []
        
        try:
            # User_Proxyå‘èµ·å¯¹è¯
            print(f"ğŸ¤– å¯åŠ¨æ™ºèƒ½ä½“åä½œ...")
            
            chat_result = self.user_proxy.initiate_chat(
                self.manager,
                message=user_query,  # User_Proxyä¼ é€’ç”¨æˆ·çš„é—®é¢˜
                max_turns=12,
                summary_method="last_msg",
            )
            
            # æå–æœ€ç»ˆå›ç­”
            final_response = ""
            
            # æŸ¥æ‰¾DeepSeek_Agentçš„æœ€ç»ˆå›ç­”
            if hasattr(chat_result, 'chat_history') and chat_result.chat_history:
                for msg in reversed(chat_result.chat_history):
                    if isinstance(msg, dict):
                        if msg.get("name") == "DeepSeek_Agent":
                            content = msg.get("content", "")
                            if content:
                                final_response = content
                                break
                    elif hasattr(msg, 'name') and msg.name == "DeepSeek_Agent":
                        if hasattr(msg, 'content'):
                            final_response = msg.content
                            break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾æœ€åä¸€ä¸ªæ™ºèƒ½ä½“çš„å›ç­”
            if not final_response and hasattr(chat_result, 'chat_history') and chat_result.chat_history:
                for msg in reversed(chat_result.chat_history):
                    if isinstance(msg, dict):
                        if msg.get("role") == "assistant" and msg.get("name") != "User_Proxy":
                            content = msg.get("content", "")
                            if content:
                                final_response = content
                                break
                    elif hasattr(msg, 'role') and msg.role == "assistant":
                        if hasattr(msg, 'name') and msg.name != "User_Proxy":
                            if hasattr(msg, 'content'):
                                final_response = msg.content
                                break
            
            print(f"âœ… AutoGenå¤šæ™ºèƒ½ä½“åä½œå®Œæˆ")
            
            # æ¸…ç†æ ‡è®°
            if final_response:
                markers = ["ã€æœ€ç»ˆå›ç­”ã€‘", "ã€è®¾è®¡å®Œæˆã€‘", "ã€éªŒè¯å®Œæˆã€‘", "ã€çŸ¥è¯†æ£€ç´¢å®Œæˆã€‘", "ã€æµç¨‹å®Œæˆã€‘"]
                for marker in markers:
                    final_response = final_response.replace(marker, "")
                final_response = final_response.strip()
            
            return final_response or "æœªèƒ½ç”Ÿæˆå®Œæ•´å›ç­”"
            
        except Exception as e:
            print(f"âŒ AutoGenæµç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return self._fallback_response(user_query)
    
    def _fallback_response(self, user_query: str) -> str:
        """å¤‡ç”¨å“åº”æ–¹æ¡ˆ - ä¿æŒåŸæœ‰é€»è¾‘"""
        try:
            # å…ˆæ£€ç´¢çŸ¥è¯†
            print("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            knowledge_results = self.knowledge_base.retrieve_knowledge(user_query, top_k=3)
            knowledge_text = "ã€ç›¸å…³çŸ¥è¯†ã€‘\n"
            if knowledge_results:
                for item in knowledge_results:
                    knowledge_text += f"- {item['content'][:150]}\n"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.DEEPSEEK_API_KEY}"
            }
            
            final_prompt = f"""{knowledge_text}

è¯·ä½œä¸ºä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦ä¸“å®¶ï¼Œè¯¦ç»†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
é—®é¢˜ï¼š{user_query}

è¦æ±‚ï¼š
1. ç»“æ„æ¸…æ™°ï¼Œä½¿ç”¨Markdownæ ¼å¼
2. æ­¥éª¤è¯¦ç»†å…·ä½“
3. åŒ…å«å¿…è¦çš„å·¥å…·å’Œæ–¹æ³•æ¨è
4. ç»™å‡ºå®Œæ•´çš„å·¥ä½œæµç¨‹"""
            
            final_payload = {
                "model": self.config.DEEPSEEK_MODEL,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦ä¸“å®¶ã€‚"},
                    {"role": "user", "content": final_prompt}
                ],
                "temperature": self.config.MODEL_CONFIG["temperature"],
                "max_tokens": 2000
            }
            
            final_response = requests.post(
                f"{self.config.DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=final_payload,
                timeout=30
            )
            
            if final_response.status_code == 200:
                return final_response.json()["choices"][0]["message"]["content"]
            else:
                return f"APIé”™è¯¯: {final_response.status_code}"
                
        except Exception as e:
            return f"å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {str(e)}"
    
    def generate_response_with_typewriter(self, user_query: str) -> Generator[str, None, str]:
        """ç”Ÿæˆå¸¦æœ‰æ‰“å­—æœºæ•ˆæœçš„å›å¤ - ä¿æŒåŸæœ‰é€»è¾‘"""
        if self.interaction_count >= self.config.MAX_INTERACTION_COUNT:
            yield "å·²è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°ã€‚è¯·é‡æ–°å¼€å§‹å¯¹è¯ã€‚\n"
            return "å·²è¾¾åˆ°æœ€å¤§äº¤äº’æ¬¡æ•°"
        
        print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        
        # ä½¿ç”¨AutoGenå·¥ä½œæµç¨‹
        print("ğŸ¤– å¯åŠ¨AutoGenå¤šæ™ºèƒ½ä½“åä½œ...")
        final_response = self._execute_autogen_workflow(user_query)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not final_response or len(final_response.strip()) < 20:
            print("âš ï¸  å›ç­”è¿‡çŸ­ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            final_response = self._fallback_response(user_query)
        
        # ç”¨æ‰“å­—æœºæ•ˆæœè¾“å‡º
        print("ğŸ“ è¾“å‡ºå›ç­”: ")
        full_response = ""
        for chunk in self._typewriter_output(final_response):
            full_response += chunk
            print(chunk, end="", flush=True)
            yield chunk
        
        print()  # æ¢è¡Œ
        
        # æ›´æ–°å†å²
        self._update_conversation_history(user_query, full_response)
        
        return full_response
    
    def generate_response(self, user_query: str) -> str:
        """ç”Ÿæˆæ™®é€šå›å¤ï¼ˆå…¼å®¹ï¼‰ - ä¿æŒåŸæœ‰é€»è¾‘"""
        full_response = ""
        for chunk in self.generate_response_with_typewriter(user_query):
            full_response += chunk
        return full_response
    
    def _update_conversation_history(self, user_query: str, response: str):
        """æ›´æ–°å¯¹è¯å†å² - ä¿æŒåŸæœ‰é€»è¾‘"""
        clean_query = self._clean_text(user_query)
        clean_response = self._clean_text(response)
        
        self.conversation_history.append({
            "role": "user", 
            "content": clean_query,
            "timestamp": time.time()
        })
        self.conversation_history.append({
            "role": "assistant", 
            "content": clean_response,
            "timestamp": time.time()
        })
        self.interaction_count += 1
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation_history) > 8:
            self.conversation_history = self.conversation_history[-8:]
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯ - ä¿æŒåŸæœ‰é€»è¾‘"""
        self.conversation_history = []
        self.interaction_count = 0
        if hasattr(self, 'group_chat'):
            self.group_chat.messages = []
        print("å¯¹è¯å†å²å·²é‡ç½®")
    
    def update_model_config(self, **kwargs):
        """æ›´æ–°æ¨¡å‹é…ç½® - ä¿æŒåŸæœ‰é€»è¾‘"""
        # æ›´æ–°é…ç½®
        if 'temperature' in kwargs:
            self.config.MODEL_CONFIG['temperature'] = kwargs['temperature']
        if 'top_p' in kwargs:
            self.config.MODEL_CONFIG['top_p'] = kwargs['top_p']
    
    def get_conversation_stats(self) -> Dict:
        """è·å–å¯¹è¯ç»Ÿè®¡ - ä¿æŒåŸæœ‰é€»è¾‘"""
        return {
            "interaction_count": self.interaction_count,
            "max_interactions": self.config.MAX_INTERACTION_COUNT,
            "history_length": len(self.conversation_history),
            "model": self.config.DEEPSEEK_MODEL,
            "active_agents": [agent.name for agent in self.agents] if hasattr(self, 'agents') else [],
            "workflow_mode": "AutoGenå¤šæ™ºèƒ½ä½“åä½œ",
            "output_mode": "æ‰“å­—æœºæ•ˆæœ"
        }